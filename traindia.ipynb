{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Dropout, Flatten\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states\n",
    "state_names = ['age', 'sex', 'BMI', 'activity', 'alcohol', 'hba', 'race_b', 'race_l',\n",
    "       'race_w']\n",
    "\n",
    "# action\n",
    "maps = '0,1,2'\n",
    "maps = maps.split(\",\")\n",
    "action_tabs = {}\n",
    "for idx, k in enumerate(maps):\n",
    "    action_tabs[idx] = k\n",
    "\n",
    "state_size = len(state_names)\n",
    "action_size = len(action_tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function assess\n",
    "class TestCallback(Callback):\n",
    "    \n",
    "    def __init__(self, model, num_epoch):\n",
    "        self.epsilon = 0.9 # for exploration\n",
    "        self.test_data = pd.read_csv(\"testdia.csv\")\n",
    "        self.rewards = []\n",
    "        self.vars = []\n",
    "        self.model = model\n",
    "        self.num_epoch = num_epoch\n",
    "        self.eval_batch = 50\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        result = []\n",
    "        states, actions, rewards = [], [], []\n",
    "        for k in range(self.test_data.shape[0]):\n",
    "            state = self.test_data.loc[k, state_names]\n",
    "            action = self.test_data.loc[k, \"treatment\"]\n",
    "            reward = self.test_data.loc[k, \"reward\"]\n",
    "            if len(states) == self.eval_batch:\n",
    "                states = np.stack(states)\n",
    "                states = states.reshape([states.shape[0], 1, states.shape[1]])\n",
    "                actions = np.array(actions)\n",
    "                rewards = np.array(rewards)\n",
    "                pred_acts = self.model.predict(states).argmax(axis=-1)\n",
    "                for idx in range(len(pred_acts)):\n",
    "                    if random.random() < self.epsilon:\n",
    "                        pred_acts[idx] = random.choice(range(len(pred_acts)))\n",
    "                ret_r = (pred_acts == actions) * rewards\n",
    "                result.append(ret_r)\n",
    "                states, actions, rewards = [], [], []\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "        self.epsilon /= (epoch* 0.05 + 1)\n",
    "        if epoch > int(self.num_epoch * 0.8):\n",
    "            self.epsilon = 0\n",
    "        print(\"epoch: \", epoch, \"reward: \", np.mean(result), \"variance: \", np.std(result))\n",
    "        self.rewards.append(np.mean(result))\n",
    "        self.vars.append(np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 256\n",
    "        self.num_epochs = 50\n",
    "        self.learning_rate = 1e-3\n",
    "        self.time_steps = 1\n",
    "        self.n_hidden = 64\n",
    "        self.n_output = action_size\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "\n",
    "    # build model\n",
    "    def _build_model(self, dropout=0.2):\n",
    "        inputs = keras.Input(shape=(self.time_steps, self.state_size), name='input2')\n",
    "        x21 = keras.layers.LSTM(self.n_hidden, return_sequences=False)(inputs)\n",
    "        x22 = keras.layers.Dense(self.n_hidden)(x21)\n",
    "        x = keras.layers.Dropout(dropout)(x22)\n",
    "        output = keras.layers.Dense(self.n_output, activation='softmax')(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=[output])\n",
    "        return model\n",
    "\n",
    "    # buffer\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory = (state, action, reward, next_state, done) \n",
    "\n",
    "    # predict\n",
    "    def act(self, state):\n",
    "        pred_acts = self.model.predict(states).argmax(axis=-1)[0]\n",
    "        return action_tabs[pred_acts]\n",
    "\n",
    "    # train\n",
    "    def learn(self):\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "        Metrics = TestCallback(self.model, self.num_epochs)\n",
    "        info = self.model.fit(self.memory[0], self.memory[1], epochs=self.num_epochs,\n",
    "                              shuffle=True, callbacks=[Metrics])\n",
    "        return Metrics\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build DQN agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# load offline data to replay buffer\n",
    "train_df = pd.read_csv(\"traindia.csv\")\n",
    "states = train_df.loc[:, state_names].values  #states\n",
    "states = states.reshape([states.shape[0], 1, states.shape[1]])\n",
    "dones = train_df.loc[:, \"episode\"].values     #dones\n",
    "actions = train_df.loc[:, \"treatment\"].values       #actions\n",
    "rewards = train_df.loc[:, \"reward\"].values    #rewards\n",
    "next_states = train_df.loc[:, state_names].values * np.reshape(dones, [-1, 1])\n",
    "next_states = np.concatenate([next_states[1:], np.zeros([1,state_size])]) #next states\n",
    "next_states = next_states.reshape([next_states.shape[0], 1, next_states.shape[-1]]) \n",
    "agent.memorize(states, actions, rewards, next_states, dones)\n",
    "\n",
    "# network train\n",
    "info = agent.learn()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "colors_comp = sns.color_palette('tab10',n_colors=4)\n",
    "\n",
    "# drawreward\n",
    "plt.figure(figsize=(7,5))\n",
    "idx = np.array(range(1, len(info.rewards) + 1)) * np.sum(dones)\n",
    "rewards = np.array(info.rewards)\n",
    "vars = np.array(info.vars)\n",
    "plt.plot(idx, info.rewards, color=colors_comp[0])\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Reward', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=16)\n",
    "plt.show()\n",
    "plt.savefig('./reward.jpg')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# drawreward + variance\n",
    "plt.figure(figsize=(7,5))\n",
    "idx = np.array(range(1, len(info.rewards) + 1)) * np.sum(dones)\n",
    "rewards = np.array(info.rewards)\n",
    "varis = np.array(info.vars)\n",
    "plt.plot(idx, info.rewards, color=colors_comp[0])\n",
    "plt.fill_between(idx, rewards-varis, rewards+varis, color=colors_comp[0], alpha=0.3)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Reward', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=16)\n",
    "plt.show()\n",
    "plt.savefig('./reward_variance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
